{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression With Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math #I import math to help with the entropy loss calculation involving logarithm \n",
    "from math import exp\n",
    "import numpy as np #I import numpy to help with array manipulation\n",
    "import sys\n",
    "\n",
    "def sigmoid(x):\n",
    "    try:\n",
    "    \n",
    "        sig= exp(x) / (1 + exp(x))\n",
    "\n",
    "    except OverflowError as err:\n",
    "        if (x>0):\n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "    \n",
    "    return sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearCombination(x, w):\n",
    "    sum = 0\n",
    "    for i in range(len(w)):\n",
    "        sum += w[i] * x[i]\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p1(x, w):\n",
    "    return sigmoid(linearCombination(w,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added a funtion to help compute the entropy loss #Entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entropy_loss(x,w,y): #This will enable me to answer the question of whether the resulting model is appropraite for the task being learned. Note that me major aim is to minimize the entropy loss. \n",
    "    sum = 0\n",
    "    for i in range(len(x)):\n",
    "        P_1 = (math.log(p1(x[i],w)))\n",
    "        P_2 = (math.log(1-p1(x[i], w)))\n",
    "        sum += -((y[i] * P_1) + ((1-y[i]) * P_2))\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "\n",
    "def gradient(X, y, w, lam):\n",
    "    sum = zeros(len(w))\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(w)):\n",
    "            sum[j] += (p1(X[i],w) - y[i]) * X[i][j] \n",
    "            \n",
    "    for j in range(len(w)):\n",
    "        sum[j] += lam * w[j]\n",
    "        \n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,lr,lam,maxIt):\n",
    "    w = zeros(len(X[0]))\n",
    "    for gen in range(maxIt):\n",
    "        grad = gradient(X,y,w,lam);\n",
    "        for i in range(len(w)):\n",
    "            w[i] = w[i] - lr * grad[i]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = gradientDescent([[1,0.3,0.1],[1,0.3,0.2],[1,0.6,0.7],[1,0.8,0.2]], [0,0,1,1], 5, 0.1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1,0.3,0.1],[1,0.3,0.2],[1,0.6,0.7],[1,0.8,0.2]]\n",
    "y =  [0,0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array(w)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_x1= p1(x[0], w) #The value of y = 0, for the given x([1,0.3,0.1])\n",
    "P_x2 = p1(x[1], w) #The value of y = 0, for the given x([1,0.3,0.2])\n",
    "P_x3  = p1(x[2], w) # The value of y = 1, for the given x([1,0.6,0.7])\n",
    "P_x4 = p1(x[3], w) #the value of y=1, for the given x([1,0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Probability of x[0] is \" +str('{0:.6f}'.format(P_x1)))\n",
    "print(\"Probability of x[1] is \" +str('{0:.6f}'.format(P_x2)))\n",
    "print(\"Probability of x[2] is \" +str('{0:.6f}'.format(P_x3)))\n",
    "print(\"Probability of x[3] is \" +str('{0:.6f}'.format(P_x1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above probability calcuations was only correct for P_x1 and P_x2 since the probabilities were less than zero and it predicted class zero (0). However, the probabilities for P_x3 and P_x4 were less than zero and it belongs to class one (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entroy_loss = Entropy_loss(x,w,y)\n",
    "print('The Entropy loss is calculated to be ' +str('{0:.4f}'.format(entroy_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy loss function shows that the resulting model is not appropriate for the task being learned. Since our aim is to minimize the entropy loss function. A model with an entropy loss of zero is said to be a perfect model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question 2\n",
    "One positive one negative regarding the choice of argument from this python statement gradientDescent([[1,0.3,0.1],[1,0.3,0.2],[1,0.6,0.7],[1,0.8,0.2]], [0,0,1,1], 5, .1, 1000).\n",
    "\n",
    "If we play around with the learning rate lr = 5, without appying the regularization then we will notice that a learning rate of 5 is very appropriate for the model as it minimize the the entropy loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "\n",
    "def gradient(X, y, w, lam):\n",
    "    sum = zeros(len(w))\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(w)):\n",
    "            sum[j] += (p1(X[i],w) - y[i]) * X[i][j] \n",
    "            \n",
    "    #for j in range(len(w)):\n",
    "        #sum[j] += lam * w[j]\n",
    "        \n",
    "    return sum \n",
    "    #case1\n",
    "    #If we change the above gradient function with this code structure we will an entropy loss of 0.004538105532145029. That is we, silence the effect of regularization (lam)\n",
    "    # Entropy loss = 0.004538105532145029. \n",
    "     \n",
    "    #case 2\n",
    "    # If We silence the effect of regularization and increase our learnig rate lr to a large extend like say lr = 50, the model begins to experience the issue overfitting and we get an entropy loss of 1.1909078857919278e-05\n",
    "    # Entropy loss = 1.1909078857919278e-05\n",
    "\n",
    "    #Case 3\n",
    "    #If we silence the effect of regularization and decrease our learning rate lr to a large extend like say lr = 0.005, the model begins to experience the issue of underfitting and our entropy loss begins to increase. we get an entropy loss of 2.021615680201623\n",
    "    # Entropy loss = 2.021615680201623\n",
    "\n",
    "    # We can conclude an lr of 5 was a very good choice and its a positive point regarding the choice of argument in the python statement. \n",
    "\n",
    "    #Please you can change the indicated parameters and arguments, and rerun the jupyter notebook to confirms this values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we play around with the regularization term (lam) while keeping every other argument same. we observe the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = gradientDescent([[1,0.3,0.1],[1,0.3,0.2],[1,0.6,0.7],[1,0.8,0.2]], [0,0,1,1], 5, 0.01, 1000)\n",
    "\n",
    "#If we play around with the regularization term (lam) while keeping every other argument same. we observe the following:\n",
    "\n",
    "#Case1\n",
    "#if we reduce the regularization term (lam) to like say lam = 0.01. Our model perfomance increases considerabily and we get en entroy loss of 0.40635578009422885.\n",
    "#  Entropy loss = 0.40635578009422885\n",
    "#if we decide to reduce further to like say lam = 0.001 our model performs even better and we get an entropy loss of 0.08866936520064983\n",
    "# Entropy loss = 0.08866936520064983\n",
    "\n",
    "#Case2\n",
    "#If we increase the regularization term lam = 0.2. model performnace get even worse. we get an entropy loss of 25.54995971940941\n",
    "# Entropy loss = 25.54995971940941\n",
    "\n",
    "# So we can conclude that our lam was one negative choice regarding argument in the python statement. \n",
    "\n",
    "#Please you can change the indicated parameters and arguments, and rerun the jupyter notebook to confirms this values "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1b4bd66619e1f63a2810c0f91c748d47860e139f5a175a9a480ebd9d4456bd5e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
